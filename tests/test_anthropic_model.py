from pathlib import Path
from unittest import mock

import pytest
from manubot_ai_editor.editor import ManuscriptEditor
from manubot_ai_editor.models import GPT3CompletionModel
from utils.dir_union import mock_unify_open


def get_editor(content_dir, config_dir):
    content_dir = content_dir.resolve(strict=True)
    config_dir = config_dir.resolve(strict=True)
    editor = ManuscriptEditor(content_dir, config_dir)
    assert isinstance(editor, ManuscriptEditor)
    return editor


# ---------
# --- live GPT version of the test, with a different prompt
# ---------

# to save on time/cost, we use a version of the phenoplier manuscript that only
# contains the first paragraph of each section
BRIEF_MANUSCRIPTS_DIR = (
    Path(__file__).parent
    / "manuscripts"
    / "phenoplier_full_only_first_para"
    / "content"
)
BRIEF_MANUSCRIPTS_CONFIG_DIR = (
    Path(__file__).parent / "manuscripts" / "phenoplier_full_only_first_para" / "ci"
)

PROMPT_PROPOGATION_CONFIG_DIR = (
    Path(__file__).parent / "config_loader_fixtures" / "prompt_gpt3_e2e"
)


@pytest.mark.cost
@mock.patch(
    "builtins.open",
    mock_unify_open(BRIEF_MANUSCRIPTS_CONFIG_DIR, PROMPT_PROPOGATION_CONFIG_DIR),
)
def test_prompts_apply_gpt3_anthropic(tmp_path):
    """
    Tests that the custom prompts are applied when actually applying
    the prompts to an LLM.

    This test uses the GPT3CompletionModel, which performs a query againts
    the live OpenAI service, thus it does incur cost. Because of that,
    this test is marked 'cost' and requires the --runcost argument to be run,
    e.g. to run just this test: `pytest --runcost -k test_prompts_apply_gpt3`.

    As with test_prompts_in_final_result above, files that have no input and
    thus no applied prompt are ignored.
    """
    me = get_editor(
        content_dir=BRIEF_MANUSCRIPTS_DIR, config_dir=BRIEF_MANUSCRIPTS_CONFIG_DIR
    )

    model = GPT3CompletionModel(
        title=me.title, keywords=me.keywords, model_provider="anthropic"
    )

    output_folder = tmp_path
    assert output_folder.exists()

    me.revise_manuscript(output_folder, model)

    # mapping of filenames to keywords, present in the prompt, to check in the
    # result. (these words were generated by https://randomwordgenerator.com/,
    # fyi, not chosen for any particular reason)
    files_to_keywords = {
        "00.front-matter.md": "testify",
        "01.abstract.md": "bottle",
        "02.introduction.md": "wound",
        # "04.00.results.md": "classroom",
        "04.05.00.results_framework.md": "secretary",
        "04.05.01.crispr.md": "army",
        "04.15.drug_disease_prediction.md": "breakdown",
        "04.20.00.traits_clustering.md": "siege",
        "05.discussion.md": "beer",
        "07.00.methods.md": "confront",
        # "10.references.md": "disability",
        "15.acknowledgements.md": "stitch",
        "50.00.supplementary_material.md": "waiter",
    }

    # check that the prompts are in the final result
    output_md_files = list(output_folder.glob("*.md"))

    for output_md_file in output_md_files:
        with open(output_md_file, "r") as f:
            content = f.read()
            assert files_to_keywords[output_md_file.name].strip() in content
